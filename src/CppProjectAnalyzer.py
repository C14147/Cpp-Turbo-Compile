import re
import json
import time
import subprocess
import fnmatch
from pathlib import Path
from collections import defaultdict, Counter
from typing import List, Dict, Any, Optional
import multiprocessing
import concurrent.futures
from config import *


class CppProjectAnalyzer:
    def __init__(self, 
                 project_path: str,
                 compiler: config.enums.Compiler = config.enums.Compiler.GCC,
                 build_system: config.enums.BuildSystem = config.enums.BuildSystem.CMAKE,
                 ignore_patterns: Optional[List[str]] = None,
                 analysis_config: Optional[AnalysisConfig] = None,
                 optimization_config: Optional[OptimizationConfig] = None):
        
        self.project_path = Path(project_path).resolve()
        self.compiler = compiler
        self.build_system = build_system
        self.ignore_patterns = ignore_patterns or []
        
        self.analysis_config = analysis_config or AnalysisConfig()
        self.optimization_config = optimization_config or OptimizationConfig()
        
        # åˆ†ææ•°æ®
        self.files = []
        self.include_graph = defaultdict(set)
        self.dependency_count = defaultdict(int)
        self.file_sizes = {}
        self.header_frequency = Counter()
        self.template_usage = Counter()
        self.circular_deps = []
        self.unused_headers = set()
        self.issues = []
        self.suggestions = []
        self.build_times_estimate = {}
        
        # ç¼–è¯‘å™¨ç‰¹å®šé…ç½®
        self.compiler_config = config.compiler.COMPILER_CONFIGS
        
        # æ„å»ºç³»ç»Ÿé…ç½®
        self.build_system_config = config.build_system.BUILD_SYSTEM_CONFIGS

    def discover_files(self) -> List[Path]:
        """å‘ç°é¡¹ç›®ä¸­çš„æ‰€æœ‰C++æ–‡ä»¶"""
        print("ğŸ” æ‰«æC++é¡¹ç›®æ–‡ä»¶...")
        
        cpp_extensions = {'.cpp', '.cc', '.cxx', '.c++', '.C'}
        header_extensions = {'.h', '.hpp', '.hh', '.hxx', '.h++', '.inl'}
        
        all_files = []
        for ext in cpp_extensions | header_extensions:
            pattern = f'**/*{ext}'
            for file_path in self.project_path.glob(pattern):
                if not self._should_ignore_file(file_path):
                    all_files.append(file_path)
                
        self.files = sorted(all_files)
        print(f"ğŸ“ æ‰¾åˆ° {len(self.files)} ä¸ªC++æ–‡ä»¶")
        return self.files
    
    def _should_ignore_file(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥è¯¥æ–‡ä»¶"""
        default_ignore_patterns = {
            'build/', 'cmake-build-', '.git/', 'third_party/', 
            'external/', 'test/', 'tests/', 'benchmark/', 'vendor/',
            'node_modules/', '__pycache__/', '.vscode/', '.vs/',
            'Debug/', 'Release/', 'x64/', 'x86/'
        }
        
        # åˆå¹¶é»˜è®¤å¿½ç•¥æ¨¡å¼å’Œç”¨æˆ·æŒ‡å®šçš„æ¨¡å¼
        all_ignore_patterns = default_ignore_patterns | set(self.ignore_patterns)
        file_str = str(file_path.relative_to(self.project_path))
        
        return any(fnmatch.fnmatch(file_str, pattern) for pattern in all_ignore_patterns)

    def analyze_project(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´é¡¹ç›®åˆ†æ"""
        start_time = time.time()
        
        print("ğŸš€ å¼€å§‹åˆ†æC++é¡¹ç›®...")
        
        # æ–‡ä»¶å‘ç°
        self.discover_files()
        
        # å¹¶è¡Œåˆ†æ
        if self.analysis_config.parallel_analysis:
            self._parallel_analyze_files()
        else:
            self._sequential_analyze_files()
        
        # é«˜çº§åˆ†æ
        if self.analysis_config.enable_circular_dep_check:
            self._detect_circular_dependencies()
            
        if self.analysis_config.enable_unused_header_check:
            self._detect_unused_headers()
            
        if self.analysis_config.enable_template_analysis:
            self._analyze_template_usage()
        
        # ç”Ÿæˆå»ºè®®
        self.generate_suggestions()
        
        # ä¼°ç®—ç¼–è¯‘æ—¶é—´
        self._estimate_build_times()
        
        elapsed_time = time.time() - start_time
        print(f"\nâ±ï¸  åˆ†æå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f} ç§’")
        
        return self._get_analysis_summary()

    def _parallel_analyze_files(self):
        """å¹¶è¡Œåˆ†ææ–‡ä»¶"""
        print("ğŸ“Š å¹¶è¡Œåˆ†ææ–‡ä»¶...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
            # åˆ†æå¤´æ–‡ä»¶åŒ…å«å…³ç³»
            future_to_file = {
                executor.submit(self._analyze_file_includes, file_path): file_path 
                for file_path in self.files
            }
            
            for future in concurrent.futures.as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    future.result(timeout=self.analysis_config.analysis_timeout)
                except concurrent.futures.TimeoutError:
                    print(f"â° åˆ†æè¶…æ—¶: {file_path}")
                except Exception as e:
                    print(f"âš ï¸  åˆ†æå¤±è´¥ {file_path}: {e}")

    def _sequential_analyze_files(self):
        """é¡ºåºåˆ†ææ–‡ä»¶"""
        print("ğŸ“Š é¡ºåºåˆ†ææ–‡ä»¶...")
        
        for file_path in self.files:
            try:
                self._analyze_file_includes(file_path)
            except Exception as e:
                print(f"âš ï¸  åˆ†æå¤±è´¥ {file_path}: {e}")

    def _analyze_file_includes(self, file_path: Path):
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„åŒ…å«å…³ç³»"""
        include_pattern = re.compile(r'^\s*#\s*include\s*[<"]([^>"]+)[">]')
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
            # åˆ†ææ–‡ä»¶å¤§å°
            self.file_sizes[file_path] = len(content)
            
            # æŸ¥æ‰¾åŒ…å«çš„å¤´æ–‡ä»¶
            includes = include_pattern.findall(content)
            for included in includes:
                # è®°å½•å¤´æ–‡ä»¶ä½¿ç”¨é¢‘ç‡
                self.header_frequency[included] += 1
                
                # å°è¯•è§£æå®é™…æ–‡ä»¶è·¯å¾„
                resolved_path = self._resolve_include_path(file_path, included)
                if resolved_path:
                    self.include_graph[file_path].add(resolved_path)
                    self.dependency_count[resolved_path] += 1
                    
            # æ£€æµ‹é—®é¢˜
            self._detect_file_issues(file_path, content)
                    
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åˆ†ææ–‡ä»¶ {file_path}: {e}")

    def _resolve_include_path(self, source_file: Path, include_name: str) -> Optional[Path]:
        """è§£æåŒ…å«è·¯å¾„åˆ°å®é™…æ–‡ä»¶"""
        # ç³»ç»Ÿå¤´æ–‡ä»¶
        if include_name.startswith('<') and '>' in include_name:
            return None
            
        # ç›¸å¯¹è·¯å¾„åŒ…å«
        if include_name.startswith('"'):
            include_name = include_name.strip('"')
            candidate = source_file.parent / include_name
            if candidate.exists():
                return candidate
                
        # åœ¨é¡¹ç›®ç›®å½•ä¸­æœç´¢
        for ext in ['', '.h', '.hpp', '.hh', '.hxx']:
            candidate_path = include_name + ext if ext else include_name
            candidate = self.project_path / candidate_path
            if candidate.exists():
                return candidate
                
            # åœ¨å­ç›®å½•ä¸­æœç´¢
            for header_file in self.project_path.rglob(candidate_path):
                if header_file.is_file():
                    return header_file
                    
        return None

    def _detect_file_issues(self, file_path: Path, content: str):
        """æ£€æµ‹æ–‡ä»¶çº§åˆ«çš„ç¼–è¯‘é—®é¢˜"""
        include_count = len(re.findall(r'^\s*#\s*include', content, re.MULTILINE))
        complexity_score = self._calculate_complexity(content)
        file_size = len(content)
        
        # è¿‡å¤šçš„å¤´æ–‡ä»¶åŒ…å«
        if include_count > self.analysis_config.max_header_includes:
            self.issues.append({
                'type': 'EXCESSIVE_INCLUDES',
                'file': str(file_path),
                'severity': config.enums.Severity.MEDIUM,
                'message': f'æ–‡ä»¶åŒ…å« {include_count} ä¸ªå¤´æ–‡ä»¶ï¼ˆè¶…è¿‡é˜ˆå€¼ {self.analysis_config.max_header_includes}ï¼‰',
                'suggestion': 'ä½¿ç”¨å‰ç½®å£°æ˜æ›¿ä»£ä¸å¿…è¦çš„å¤´æ–‡ä»¶åŒ…å«ï¼Œè€ƒè™‘ä½¿ç”¨PIMPLæ¨¡å¼'
            })
        
        # é«˜å¤æ‚æ€§æ–‡ä»¶
        if complexity_score > self.analysis_config.max_file_complexity:
            self.issues.append({
                'type': 'HIGH_COMPLEXITY',
                'file': str(file_path),
                'severity': config.enums.Severity.HIGH,
                'message': f'æ–‡ä»¶å¤æ‚æ€§è¾ƒé«˜ (åˆ†æ•°: {complexity_score})ï¼Œå¯èƒ½æ˜¾è‘—å¢åŠ ç¼–è¯‘æ—¶é—´',
                'suggestion': 'è€ƒè™‘æ‹†åˆ†æ–‡ä»¶æˆ–å‡å°‘æ¨¡æ¿ä½¿ç”¨'
            })
        
        # å¤§å‹å¤´æ–‡ä»¶
        if file_path.suffix in {'.h', '.hpp', '.hh'} and file_size > self.analysis_config.max_header_size:
            self.issues.append({
                'type': 'LARGE_HEADER',
                'file': str(file_path),
                'severity': config.enums.Severity.MEDIUM,
                'message': f'å¤´æ–‡ä»¶è¾ƒå¤§ ({file_size} å­—èŠ‚)ï¼Œå½±å“åŒ…å«å®ƒçš„æ‰€æœ‰ç¼–è¯‘å•å…ƒ',
                'suggestion': 'æ‹†åˆ†å¤´æ–‡ä»¶æˆ–ä½¿ç”¨å‰ç½®å£°æ˜ï¼Œè€ƒè™‘ä½¿ç”¨PIMPLæ¨¡å¼'
            })

    def _calculate_complexity(self, content: str) -> int:
        """è®¡ç®—æ–‡ä»¶çš„å¤æ‚æ€§åˆ†æ•°"""
        complexity = 0
        
        # æ¨¡æ¿ä½¿ç”¨
        if self.analysis_config.enable_template_analysis:
            template_pattern = re.compile(r'template\s*<[^>]*>')
            complexity += len(template_pattern.findall(content)) * 3
            
            # æ¨¡æ¿ç‰¹åŒ–/åç‰¹åŒ–
            template_specialization = re.compile(r'template\s*<>\s*[^;{]+')
            complexity += len(template_specialization.findall(content)) * 2
        
        # å¤´æ–‡ä»¶åŒ…å«æ•°é‡
        include_pattern = re.compile(r'^\s*#\s*include', re.MULTILINE)
        complexity += len(include_pattern.findall(content))
        
        # ç±»å®šä¹‰
        class_pattern = re.compile(r'(class|struct)\s+\w+')
        complexity += len(class_pattern.findall(content)) * 2
        
        # å‡½æ•°å®šä¹‰
        function_pattern = re.compile(r'(\w+)\s+\w+\s*\([^)]*\)\s*(\{|\[\[[^\]]*\]\])')
        complexity += len(function_pattern.findall(content))
        
        # å®å®šä¹‰
        macro_pattern = re.compile(r'^\s*#\s*define\s+\w+', re.MULTILINE)
        complexity += len(macro_pattern.findall(content)) * 0.5
        
        return int(complexity)

    def _detect_circular_dependencies(self):
        """æ£€æµ‹å¾ªç¯ä¾èµ–"""
        print("ğŸ”„ æ£€æµ‹å¾ªç¯ä¾èµ–...")
        
        visited = set()
        recursion_stack = set()
        
        def dfs(file_path):
            if file_path in recursion_stack:
                # æ‰¾åˆ°å¾ªç¯ä¾èµ–
                cycle_start = list(recursion_stack).index(file_path)
                cycle = list(recursion_stack)[cycle_start:]
                self.circular_deps.append(cycle)
                return
            
            if file_path in visited:
                return
            
            visited.add(file_path)
            recursion_stack.add(file_path)
            
            for dependency in self.include_graph.get(file_path, set()):
                dfs(dependency)
            
            recursion_stack.remove(file_path)
        
        for file_path in self.files:
            if file_path not in visited:
                dfs(file_path)
        
        if self.circular_deps:
            for i, cycle in enumerate(self.circular_deps):
                self.issues.append({
                    'type': 'CIRCULAR_DEPENDENCY',
                    'file': f"Cycle {i+1}",
                    'severity': config.enums.Severity.HIGH,
                    'message': f'æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {" -> ".join(str(f) for f in cycle)}',
                    'suggestion': 'æ‰“ç ´å¾ªç¯ä¾èµ–ï¼Œä½¿ç”¨å‰ç½®å£°æ˜æˆ–é‡æ„ä»£ç ç»“æ„'
                })

    def _detect_unused_headers(self):
        """æ£€æµ‹æœªä½¿ç”¨çš„å¤´æ–‡ä»¶"""
        print("ğŸ” æ£€æµ‹æœªä½¿ç”¨çš„å¤´æ–‡ä»¶...")
        
        # æ‰€æœ‰è¢«åŒ…å«çš„å¤´æ–‡ä»¶
        included_headers = set()
        for dependencies in self.include_graph.values():
            included_headers.update(dependencies)
        
        # é¡¹ç›®ä¸­çš„æ‰€æœ‰å¤´æ–‡ä»¶
        all_headers = {f for f in self.files if f.suffix in {'.h', '.hpp', '.hh'}}
        
        # æ‰¾åˆ°æœªè¢«åŒ…å«çš„å¤´æ–‡ä»¶
        self.unused_headers = all_headers - included_headers
        
        for header in self.unused_headers:
            self.issues.append({
                'type': 'UNUSED_HEADER',
                'file': str(header),
                'severity': config.enums.Severity.LOW,
                'message': 'å¤´æ–‡ä»¶æœªè¢«ä»»ä½•æºæ–‡ä»¶åŒ…å«',
                'suggestion': 'è€ƒè™‘åˆ é™¤æˆ–æ£€æŸ¥æ˜¯å¦éœ€è¦æ­¤å¤´æ–‡ä»¶'
            })

    def _analyze_template_usage(self):
        """åˆ†ææ¨¡æ¿ä½¿ç”¨æƒ…å†µ"""
        print("ğŸ“ åˆ†ææ¨¡æ¿ä½¿ç”¨...")
        
        template_patterns = [
            (r'template\s*<[^>]*>', "æ¨¡æ¿å£°æ˜"),
            (r'std::\w+\s*<[^>]*>', "STLæ¨¡æ¿"),
            (r'boost::\w+\s*<[^>]*>', "Boostæ¨¡æ¿"),
        ]
        
        for file_path in self.files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                for pattern, description in template_patterns:
                    matches = re.findall(pattern, content)
                    if matches:
                        self.template_usage[description] += len(matches)
            except Exception:
                continue

    def _estimate_build_times(self):
        """ä¼°ç®—æ„å»ºæ—¶é—´"""
        print("â±ï¸  ä¼°ç®—æ„å»ºæ—¶é—´...")
        
        base_compile_time_per_line = 0.001  # ç§’/è¡Œï¼ˆç»éªŒå€¼ï¼‰
        
        for file_path in self.files:
            if file_path.suffix in {'.cpp', '.cc', '.cxx'}:
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        lines = len(f.readlines())
                    
                    complexity = self._calculate_complexity(open(file_path).read())
                    dependency_penalty = len(self.include_graph.get(file_path, set())) * 0.1
                    
                    # ä¼°ç®—ç¼–è¯‘æ—¶é—´
                    estimated_time = (lines * base_compile_time_per_line * 
                                    (1 + complexity * 0.01) * (1 + dependency_penalty))
                    
                    self.build_times_estimate[file_path] = estimated_time
                    
                except Exception:
                    self.build_times_estimate[file_path] = 0

    def generate_pch_header(self, pch_name: str = None) -> Path:
        """ç”Ÿæˆé¢„ç¼–è¯‘å¤´æ–‡ä»¶"""
        pch_name = pch_name or self.optimization_config.pch_name
        print(f"ğŸ¯ ç”Ÿæˆé¢„ç¼–è¯‘å¤´æ–‡ä»¶: {pch_name}")
        
        # è·å–æœ€å¸¸ç”¨çš„å¤´æ–‡ä»¶
        max_headers = self.analysis_config.pch_max_headers
        common_headers = self.header_frequency.most_common(max_headers)
        
        pch_content = f"""// pch.h - Generated Precompiled Header
// Generated by C++ Project Analyzer (Cpp-Turbo-Compile)
// Generate Time: {time.strftime('%Y-%m-%d %H:%M:%S')}
// config.enums.Compiler: {self.compiler.value}
// Build System: {self.build_system.value}
// Project: {self.project_path.name}

#pragma once

// The most useful system headers
"""
        
        # åˆ†ç¦»ç³»ç»Ÿå¤´æ–‡ä»¶å’Œé¡¹ç›®å¤´æ–‡ä»¶
        system_headers = []
        project_headers = []
        
        for header, count in common_headers:
            if header.startswith('<') or '/' in header or any(header.endswith(ext) for ext in ['.h', '.hpp']):
                if any(pattern in header for pattern in ['<', '>', '.h']):
                    system_headers.append((header, count))
            else:
                project_headers.append((header, count))
        
        # æ·»åŠ ç³»ç»Ÿå¤´æ–‡ä»¶
        for header, count in system_headers:
            if header.startswith('<'):
                pch_content += f"#include {header}  // times: {count}\n"
            else:
                pch_content += f'#include "{header}"  // times: {count}\n'
        
        # æ·»åŠ é¡¹ç›®å¤´æ–‡ä»¶
        if project_headers:
            pch_content += "\n// Project headers\n"
            for header, count in project_headers:
                pch_content += f'#include "{header}"  // time: {count}\n'
        
        compiler_config = self.compiler_config.get(self.compiler, self.compiler_config[config.enums.Compiler.GCC])
        
        pch_content += config.pch.PCH_SPECIAL_OPT
        
        pch_file = self.project_path / pch_name
        with open(pch_file, 'w', encoding='utf-8') as f:
            f.write(pch_content)
        
        print(f"âœ… é¢„ç¼–è¯‘å¤´æ–‡ä»¶å·²ç”Ÿæˆ: {pch_file}")
        return pch_file

    def compile_pch(self, pch_name: str = None) -> bool:
        """ç¼–è¯‘é¢„ç¼–è¯‘å¤´æ–‡ä»¶"""
        pch_name = pch_name or self.optimization_config.pch_name
        print(f"ğŸ”¨ ç¼–è¯‘é¢„ç¼–è¯‘å¤´æ–‡ä»¶: {pch_name}")
        
        pch_file = self.project_path / pch_name
        if not pch_file.exists():
            print(f"âŒ é¢„ç¼–è¯‘å¤´æ–‡ä»¶ä¸å­˜åœ¨: {pch_file}")
            return False
        
        compiler_config = self.compiler_config.get(self.compiler, self.compiler_config[config.enums.Compiler.GCC])
        
        try:
            if self.compiler in [config.enums.Compiler.GCC, config.enums.Compiler.CLANG, config.enums.Compiler.ICC]:
                # GCC/Clang/ICC PCHç¼–è¯‘
                pch_output = pch_file.with_suffix(compiler_config["pch_ext"])
                cmd = [
                    self.compiler.value, 
                    *compiler_config["pch_flags"],
                    "-std=c++17",
                    "-O2",
                    "-I.", f"-I{self.project_path}",
                    "-o", str(pch_output),
                    str(pch_file)
                ]
                
                # æ·»åŠ ç‰¹å®šç¼–è¯‘å™¨ä¼˜åŒ–
                if self.optimization_config.enable_lto:
                    cmd.append("-flto")
                
                result = subprocess.run(cmd, cwd=self.project_path, 
                                      capture_output=True, text=True)
                
                if result.returncode == 0:
                    print(f"âœ… é¢„ç¼–è¯‘å¤´æ–‡ä»¶ç¼–è¯‘æˆåŠŸ: {pch_output}")
                    return True
                else:
                    print(f"âŒ é¢„ç¼–è¯‘å¤´æ–‡ä»¶ç¼–è¯‘å¤±è´¥: {result.stderr}")
                    return False
                    
            elif self.compiler == config.enums.Compiler.MSVC:
                # MSVC PCHç¼–è¯‘ (ç®€åŒ–ç‰ˆæœ¬)
                print("â„¹ï¸  MSVC PCHç¼–è¯‘éœ€è¦Visual Studioç¯å¢ƒ")
                print("   è¯·æ‰‹åŠ¨åœ¨Visual Studioä¸­é…ç½®é¢„ç¼–è¯‘å¤´æ–‡ä»¶")
                return False
                
        except Exception as e:
            print(f"âŒ ç¼–è¯‘é¢„ç¼–è¯‘å¤´æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False

    def generate_build_configurations(self) -> Dict[str, str]:
        """ç”Ÿæˆæ„å»ºç³»ç»Ÿé…ç½®"""
        print(f"âš™ï¸  ç”Ÿæˆ {self.build_system.value} é…ç½®")
        
        config_generators = {
            config.enums.BuildSystem.CMAKE: self._generate_cmake_config,
            config.enums.BuildSystem.QMAKE: self._generate_qmake_config,
            config.enums.BuildSystem.NINJA: self._generate_ninja_config,
            config.enums.BuildSystem.MSBUILD: self._generate_msbuild_config,
            config.enums.BuildSystem.MAKE: self._generate_make_config,
            config.enums.BuildSystem.BAZEL: self._generate_bazel_config,
            config.enums.BuildSystem.MESON: self._generate_meson_config
        }
        
        configs = {}
        for build_sys, generator in config_generators.items():
            configs[build_sys.value] = generator()
        
        return configs

    def _generate_cmake_config(self) -> str:
        """ç”ŸæˆCMakeé…ç½®"""
        pch_config = ""
        if self.optimization_config.generate_pch:
            pch_config = f"""
# é¢„ç¼–è¯‘å¤´æ–‡ä»¶é…ç½®
target_precompile_headers(${{PROJECT_NAME}} PRIVATE {self.optimization_config.pch_name})
"""
        
        lto_config = ""
        if self.optimization_config.enable_lto:
            lto_config = """
# é“¾æ¥æ—¶ä¼˜åŒ–
include(CheckIPOSupported)
check_ipo_supported(RESULT IPO_SUPPORTED OUTPUT IPO_ERROR)
if(IPO_SUPPORTED)
    set(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)
endif()
"""
        
        parallel_config = ""
        if self.optimization_config.parallel_build:
            parallel_config = """
# å¹¶è¡Œç¼–è¯‘
if(NOT MSVC)
    find_program(CCACHE_PROGRAM ccache)
    if(CCACHE_PROGRAM)
        set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM})
        set_property(GLOBAL PROPERTY RULE_LAUNCH_LINK ${CCACHE_PROGRAM})
    endif()
endif()
"""
        
        config = f"""
# CMakeé…ç½® - ç”±C++é¡¹ç›®åˆ†æå™¨è‡ªåŠ¨ç”Ÿæˆ
# ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}

# ç¼–è¯‘å™¨ä¼˜åŒ–
if(CMAKE_BUILD_TYPE STREQUAL "Release")
    target_compile_options(${{PROJECT_NAME}} PRIVATE
        -O3
        -DNDEBUG
        -march=native
    )
else()
    target_compile_options(${{PROJECT_NAME}} PRIVATE
        -O0
        -g3
        -Wall
        -Wextra
    )
endif()

{pch_config}
{lto_config}
{parallel_config}

# å¹¶è¡Œæ„å»º
set(CMAKE_BUILD_PARALLEL_LEVEL ${{CMAKE_SYSTEM_PROCESSOR_COUNT}})
"""
        return config

    def _generate_qmake_config(self) -> str:
        """ç”ŸæˆQMakeé…ç½®"""
        return f"""
# QMakeé…ç½® - è‡ªåŠ¨ç”Ÿæˆ
CONFIG += c++17 precompile_header
PRECOMPILED_HEADER = {self.optimization_config.pch_name}

# ç¼–è¯‘å™¨ä¼˜åŒ–
QMAKE_CXXFLAGS_RELEASE += -O3 -march=native
QMAKE_CXXFLAGS_DEBUG += -O0 -g

# å¹¶è¡Œç¼–è¯‘
unix {{
    QMAKE_CXXFLAGS += -j$$system(nproc)
}}

# MSVCç‰¹å®šè®¾ç½®
win32:msvc {{
    PRECOMPILED_HEADER = {self.optimization_config.pch_name}
}}
"""

    def _generate_ninja_config(self) -> str:
        """ç”ŸæˆNinjaé…ç½®"""
        return f"""
# Ninjaæ„å»ºé…ç½® - éœ€è¦é…åˆCMakeä½¿ç”¨
# åœ¨CMakeä¸­å¯ç”¨é¢„ç¼–è¯‘å¤´æ–‡ä»¶ï¼ŒNinjaä¼šè‡ªåŠ¨å¤„ç†å¹¶è¡Œç¼–è¯‘

# æ‰‹åŠ¨ç¼–è¯‘PCHçš„å‘½ä»¤:
# {self.compiler.value} -x c++-header {self.optimization_config.pch_name} -o {self.optimization_config.pch_name}{self.compiler_config.get(self.compiler, {}).get('pch_ext', '.gch')}

# ä¼˜åŒ–æ„å»º
pool = console
builddir = build

# å¹¶è¡Œæ„å»º
ninja_required_version = 1.7
"""

    def _generate_msbuild_config(self) -> str:
        """ç”ŸæˆMSBuildé…ç½®"""
        return f"""
<!-- MSBuildé…ç½® - è‡ªåŠ¨ç”Ÿæˆ -->
<PropertyGroup>
  <PrecompiledHeader>Create</PrecompiledHeader>
  <PrecompiledHeaderFile>{self.optimization_config.pch_name}</PrecompiledHeaderFile>
  <MultiProcessorCompilation>true</MultiProcessorCompilation>
  <Optimization>MaxSpeed</Optimization>
  <IntrinsicFunctions>true</IntrinsicFunctions>
  <FunctionLevelLinking>true</FunctionLevelLinking>
</PropertyGroup>

<ItemDefinitionGroup>
  <ClCompile>
    <PrecompiledHeader>Use</PrecompiledHeader>
    <WarningLevel>Level4</WarningLevel>
    <Optimization>MaxSpeed</Optimization>
  </ClCompile>
</ItemDefinitionGroup>
"""

    def _generate_make_config(self) -> str:
        """ç”ŸæˆMakefileé…ç½®"""
        return f"""
# Makefileé…ç½® - è‡ªåŠ¨ç”Ÿæˆ
CXX = {self.compiler.value}
CXXFLAGS = -std=c++17 -I. -Wall -Wextra
PCH_HEADER = {self.optimization_config.pch_name}
PCH_FILE = $(PCH_HEADER){self.compiler_config.get(self.compiler, {}).get('pch_ext', '.gch')}

# é¢„ç¼–è¯‘å¤´æ–‡ä»¶è§„åˆ™
$(PCH_FILE): $(PCH_HEADER)
\t$(CXX) -x c++-header $(CXXFLAGS) $(PCH_HEADER) -o $(PCH_FILE)

# åŒ…å«PCHçš„ç¼–è¯‘è§„åˆ™
%.o: %.cpp $(PCH_FILE)
\t$(CXX) $(CXXFLAGS) -include $(PCH_HEADER) -c $< -o $@

# å¹¶è¡Œç¼–è¯‘
JOBS := $(shell nproc 2>/dev/null || echo 4)

build: $(PCH_FILE)
\t$(MAKE) -j$(JOBS) all

# æ¸…ç†
clean:
\trm -f $(PCH_FILE) *.o
"""

    def _generate_bazel_config(self) -> str:
        """ç”ŸæˆBazelé…ç½®"""
        return """
# Bazelé…ç½® - è‡ªåŠ¨ç”Ÿæˆ
# åœ¨BUILDæ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®:

# cc_library(
#     name = "pch",
#     hdrs = ["pch.h"],
#     copts = ["-include", "pch.h"],
# )

# å¹¶è¡Œæ„å»º
build --jobs=$(nproc 2>/dev/null || echo 4)
build --compilation_mode=opt
build --copt=-O3
"""

    def _generate_meson_config(self) -> str:
        """ç”ŸæˆMesoné…ç½®"""
        return f"""
# Mesoné…ç½® - è‡ªåŠ¨ç”Ÿæˆ
project('{self.project_path.name}', 'cpp',
  version : '1.0',
  default_options : [
    'warning_level=3',
    'cpp_std=c++17',
    'buildtype=release',
    'optimization=3',
    'b_lto=true',
    'b_pch=true'
  ]
)

# é¢„ç¼–è¯‘å¤´æ–‡ä»¶
pch = declare_dependency(
  compile_args: ['-include', '{self.optimization_config.pch_name}']
)

# å¹¶è¡Œæ„å»º
meson.add_install_script('post_install.py')
"""

    def generate_suggestions(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print("ğŸ’¡ ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        self._suggest_forward_declarations()
        self._suggest_pimpl_pattern()
        self._suggest_unified_headers()
        self._suggest_build_optimizations()
        self._suggest_compiler_specific_optimizations()
        self._suggest_code_restructuring()
        self._suggest_caching_strategies()

    def _suggest_forward_declarations(self):
        """å»ºè®®ä½¿ç”¨å‰ç½®å£°æ˜"""
        highly_included_headers = [
            header for header, count in self.dependency_count.items() 
            if count > 5 and header.suffix in {'.h', '.hpp', '.hh'}
        ]
        
        for header in highly_included_headers:
            self.suggestions.append({
                'type': 'FORWARD_DECLARATION',
                'target': str(header),
                'priority': config.enums.Severity.HIGH,
                'description': f'è¯¥å¤´æ–‡ä»¶è¢« {self.dependency_count[header]} ä¸ªæ–‡ä»¶åŒ…å«ï¼Œè€ƒè™‘ä½¿ç”¨å‰ç½®å£°æ˜',
                'action': f'åœ¨ä¾èµ–æ­¤å¤´æ–‡ä»¶çš„æºæ–‡ä»¶ä¸­ä½¿ç”¨ class {header.stem}; æ›¿ä»£åŒ…å«'
            })

    def _suggest_pimpl_pattern(self):
        """å»ºè®®ä½¿ç”¨PIMPLæ¨¡å¼"""
        large_headers = [
            file for file, size in self.file_sizes.items()
            if size > 15000 and file.suffix in {'.h', '.hpp', '.hh'}
        ]
        
        for header in large_headers:
            self.suggestions.append({
                'type': 'PIMPL_PATTERN',
                'target': str(header),
                'priority': config.enums.Severity.MEDIUM,
                'description': f'å¤§å‹å¤´æ–‡ä»¶ {header.stem} é€‚åˆä½¿ç”¨PIMPLæ¨¡å¼',
                'action': 'å®ç°Pointer to Implementationæ¨¡å¼éšè—å®ç°ç»†èŠ‚'
            })

    def _suggest_unified_headers(self):
        """å»ºè®®ç»Ÿä¸€å¤´æ–‡ä»¶"""
        header_files = [f for f in self.files if f.suffix in {'.h', '.hpp', '.hh'}]
        
        for header in header_files:
            content = ""
            try:
                with open(header, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            except:
                continue
                
            # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯å‰ç½®å£°æ˜
            forward_decls = len(re.findall(r'^\s*(class|struct)\s+\w+;', content, re.MULTILINE))
            includes = len(re.findall(r'^\s*#\s*include', content, re.MULTILINE))
            
            if forward_decls > 5 and includes < 3:
                self.suggestions.append({
                    'type': 'UNIFIED_HEADER',
                    'target': str(header),
                    'priority': config.enums.Severity.LOW,
                    'description': f'æ£€æµ‹åˆ°å‰ç½®å£°æ˜å¤´æ–‡ä»¶ï¼Œå¯ç»Ÿä¸€ç®¡ç†ç±»å‹å£°æ˜',
                    'action': 'è€ƒè™‘å°†æ­¤æ–‡ä»¶ä½œä¸ºé¡¹ç›®çš„å‰å‘å£°æ˜ç»Ÿä¸€å…¥å£'
                })

    def _suggest_build_optimizations(self):
        """å»ºè®®æ„å»ºä¼˜åŒ–"""
        cpu_count = multiprocessing.cpu_count()
        
        build_suggestions = [
            {
                'type': 'BUILD_OPTIMIZATION',
                'target': 'PROJECT',
                'priority': config.enums.Severity.HIGH,
                'description': f'ä½¿ç”¨{self.build_system.value}å¹¶è¡Œç¼–è¯‘',
                'action': f'ä½¿ç”¨ {self._get_parallel_build_command()} è¿›è¡Œå¹¶è¡Œç¼–è¯‘'
            }
        ]
        
        if self.optimization_config.cache_compilation:
            build_suggestions.append({
                'type': 'BUILD_OPTIMIZATION', 
                'target': 'PROJECT',
                'priority': config.enums.Severity.MEDIUM,
                'description': 'ä½¿ç”¨ccache/sccacheåŠ é€Ÿç¼–è¯‘',
                'action': 'å®‰è£…å¹¶é…ç½®ccache: sudo apt install ccache && export CC="ccache gcc"'
            })
            
        if self.optimization_config.unity_build:
            build_suggestions.append({
                'type': 'BUILD_OPTIMIZATION',
                'target': 'PROJECT',
                'priority': config.enums.Severity.MEDIUM,
                'description': 'ä½¿ç”¨Unity Buildå‡å°‘ç¼–è¯‘å•å…ƒ',
                'action': 'åˆå¹¶å¤šä¸ªæºæ–‡ä»¶åˆ°ä¸€ä¸ªç¼–è¯‘å•å…ƒä»¥å‡å°‘é‡å¤åŒ…å«'
            })
        
        self.suggestions.extend(build_suggestions)

    def _suggest_compiler_specific_optimizations(self):
        """å»ºè®®ç¼–è¯‘å™¨ç‰¹å®šä¼˜åŒ–"""
        compiler_suggestions = {
            config.enums.Compiler.GCC: [
                {
                    'type': 'COMPILER_OPTIMIZATION',
                    'target': 'GCC',
                    'priority': config.enums.Severity.MEDIUM,
                    'description': 'ä½¿ç”¨é“¾æ¥æ—¶ä¼˜åŒ–(LTO)',
                    'action': 'æ·»åŠ ç¼–è¯‘é€‰é¡¹: -flto -O2'
                },
                {
                    'type': 'COMPILER_OPTIMIZATION',
                    'target': 'GCC', 
                    'priority': config.enums.Severity.LOW,
                    'description': 'ä½¿ç”¨PGOä¼˜åŒ–',
                    'action': 'åˆ†é˜¶æ®µç¼–è¯‘: 1) -fprofile-generate 2) è¿è¡Œç¨‹åº 3) -fprofile-use'
                }
            ],
            config.enums.Compiler.CLANG: [
                {
                    'type': 'COMPILER_OPTIMIZATION', 
                    'target': 'Clang',
                    'priority': config.enums.Severity.MEDIUM,
                    'description': 'ä½¿ç”¨ThinLTOä¼˜åŒ–',
                    'action': 'æ·»åŠ ç¼–è¯‘é€‰é¡¹: -flto=thin -O2'
                }
            ],
            config.enums.Compiler.MSVC: [
                {
                    'type': 'COMPILER_OPTIMIZATION',
                    'target': 'MSVC',
                    'priority': config.enums.Severity.MEDIUM, 
                    'description': 'å¯ç”¨å…¨ç¨‹åºä¼˜åŒ–',
                    'action': 'æ·»åŠ ç¼–è¯‘é€‰é¡¹: /GL /O2'
                }
            ],
            config.enums.Compiler.ICC: [
                {
                    'type': 'COMPILER_OPTIMIZATION',
                    'target': 'ICC',
                    'priority': config.enums.Severity.MEDIUM,
                    'description': 'ä½¿ç”¨Interprocedural Optimization',
                    'action': 'æ·»åŠ ç¼–è¯‘é€‰é¡¹: -ipo -O3'
                }
            ]
        }
        
        self.suggestions.extend(compiler_suggestions.get(self.compiler, []))

    def _suggest_code_restructuring(self):
        """å»ºè®®ä»£ç é‡æ„"""
        # åŸºäºç¼–è¯‘æ—¶é—´ä¼°ç®—çš„é‡æ„å»ºè®®
        slow_files = sorted(
            [(f, t) for f, t in self.build_times_estimate.items() if t > 1.0],
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        for file_path, est_time in slow_files:
            self.suggestions.append({
                'type': 'CODE_RESTRUCTURING',
                'target': str(file_path),
                'priority': config.enums.Severity.MEDIUM,
                'description': f'æ–‡ä»¶ç¼–è¯‘æ—¶é—´é¢„ä¼°è¾ƒé•¿ ({est_time:.2f}s)',
                'action': 'è€ƒè™‘æ‹†åˆ†æ–‡ä»¶æˆ–ä¼˜åŒ–åŒ…å«å…³ç³»'
            })

    def _suggest_caching_strategies(self):
        """å»ºè®®ç¼“å­˜ç­–ç•¥"""
        if self.optimization_config.cache_compilation:
            self.suggestions.append({
                'type': 'CACHING_STRATEGY',
                'target': 'PROJECT',
                'priority': config.enums.Severity.MEDIUM,
                'description': 'é…ç½®åˆ†å¸ƒå¼ç¼–è¯‘ç¼“å­˜',
                'action': 'è€ƒè™‘ä½¿ç”¨distccæˆ–iceccè¿›è¡Œåˆ†å¸ƒå¼ç¼–è¯‘'
            })

    def _get_parallel_build_command(self) -> str:
        """è·å–å¹¶è¡Œæ„å»ºå‘½ä»¤"""
        commands = {
            config.enums.BuildSystem.CMAKE: "cmake --build . --parallel",
            config.enums.BuildSystem.MAKE: "make -j$(nproc)",
            config.enums.BuildSystem.NINJA: "ninja -j$(nproc)", 
            config.enums.BuildSystem.QMAKE: "make -j$(nproc)",
            config.enums.BuildSystem.MSBUILD: "msbuild /m",
            config.enums.BuildSystem.BAZEL: "bazel build --jobs=$(nproc)",
            config.enums.BuildSystem.MESON: "ninja -j$(nproc)"  # Mesoné€šå¸¸ä½¿ç”¨Ninjaä½œä¸ºåç«¯
        }
        return commands.get(self.build_system, "make -j$(nproc)")

    def _get_analysis_summary(self) -> Dict[str, Any]:
        """è·å–åˆ†ææ‘˜è¦"""
        header_files = [f for f in self.files if f.suffix in {'.h', '.hpp', '.hh'}]
        source_files = [f for f in self.files if f.suffix in {'.cpp', '.cc', '.cxx'}]
        
        total_estimated_build_time = sum(self.build_times_estimate.values())
        
        return {
            'project_info': {
                'path': str(self.project_path),
                'compiler': self.compiler.value,
                'build_system': self.build_system.value,
                'total_files': len(self.files),
                'header_files': len(header_files),
                'source_files': len(source_files),
                'estimated_build_time': total_estimated_build_time
            },
            'analysis_results': {
                'issues_found': len(self.issues),
                'suggestions': len(self.suggestions),
                'circular_deps': len(self.circular_deps),
                'unused_headers': len(self.unused_headers),
                'most_used_headers': dict(self.header_frequency.most_common(10)),
                'template_usage': dict(self.template_usage)
            }
        }

    def generate_report(self, output_file: Optional[str] = None, format: str = "text") -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        summary = self._get_analysis_summary()
        
        if format == "json":
            report = {
                'summary': summary,
                'issues': self.issues,
                'suggestions': self.suggestions,
                'detailed_analysis': {
                    'file_complexity': {
                        str(f): self._calculate_complexity(open(f).read()) 
                        for f in self.files if f.exists()
                    },
                    'build_time_estimates': {
                        str(f): t for f, t in self.build_times_estimate.items()
                    }
                }
            }
            
            if output_file:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                print(f"ğŸ’¾ JSONæŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
            
            return report
        
        else:  # text format
            print("\n" + "="*70)
            print("ğŸ“Š C++é¡¹ç›®ç¼–è¯‘ä¼˜åŒ–åˆ†ææŠ¥å‘Š")
            print("="*70)
            
            proj_info = summary['project_info']
            analysis_results = summary['analysis_results']
            
            print(f"\nğŸ“ˆ é¡¹ç›®ç»Ÿè®¡:")
            print(f"   é¡¹ç›®è·¯å¾„: {proj_info['path']}")
            print(f"   ç¼–è¯‘å™¨: {proj_info['compiler']}")
            print(f"   æ„å»ºç³»ç»Ÿ: {proj_info['build_system']}")
            print(f"   æ€»æ–‡ä»¶æ•°: {proj_info['total_files']}")
            print(f"   å¤´æ–‡ä»¶: {proj_info['header_files']}")
            print(f"   æºæ–‡ä»¶: {proj_info['source_files']}")
            print(f"   é¢„ä¼°ç¼–è¯‘æ—¶é—´: {proj_info['estimated_build_time']:.2f}s")
            
            # æ˜¾ç¤ºæœ€å¸¸ç”¨çš„å¤´æ–‡ä»¶
            if analysis_results['most_used_headers']:
                print(f"\nğŸ“‹ æœ€å¸¸ç”¨çš„å¤´æ–‡ä»¶:")
                for header, count in analysis_results['most_used_headers'].items():
                    print(f"   {header}: {count} æ¬¡")
            
            # é—®é¢˜æŠ¥å‘Š
            if self.issues:
                print(f"\nâŒ æ£€æµ‹åˆ° {len(self.issues)} ä¸ªé—®é¢˜:")
                for issue in self.issues:
                    severity_icon = {
                        config.enums.Severity.LOW: "ğŸ”µ",
                        config.enums.Severity.MEDIUM: "ğŸŸ¡", 
                        config.enums.Severity.HIGH: "ğŸ”´"
                    }.get(issue['severity'], "âšª")
                    
                    print(f"   {severity_icon} [{issue['severity'].name}] {issue['file']}")
                    print(f"       {issue['message']}")
                    print(f"       ğŸ’¡ å»ºè®®: {issue['suggestion']}")
            else:
                print(f"\nâœ… æœªå‘ç°ä¸¥é‡ç¼–è¯‘é—®é¢˜")
            
            # ä¼˜åŒ–å»ºè®®
            if self.suggestions:
                print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®® ({len(self.suggestions)} ä¸ª):")
                
                # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
                high_priority = [s for s in self.suggestions if s['priority'] == config.enums.Severity.HIGH]
                medium_priority = [s for s in self.suggestions if s['priority'] == config.enums.Severity.MEDIUM]
                low_priority = [s for s in self.suggestions if s['priority'] == config.enums.Severity.LOW]
                
                if high_priority:
                    print("\n   ğŸ”´ é«˜ä¼˜å…ˆçº§:")
                    for suggestion in high_priority:
                        print(f"      {suggestion['description']}")
                        print(f"      â†’ {suggestion['action']}")
                
                if medium_priority:
                    print("\n   ğŸŸ¡ ä¸­ä¼˜å…ˆçº§:")  
                    for suggestion in medium_priority:
                        print(f"      {suggestion['description']}")
                        print(f"      â†’ {suggestion['action']}")
                
                if low_priority:
                    print("\n   ğŸ”µ ä½ä¼˜å…ˆçº§:")
                    for suggestion in low_priority:
                        print(f"      {suggestion['description']}")
                        print(f"      â†’ {suggestion['action']}")
            
            # ä¿å­˜æŠ¥å‘Š
            if output_file and format == "text":
                self._save_text_report(output_file, summary)
            
            return summary

    def _save_text_report(self, output_file: str, summary: Dict[str, Any]):
        """ä¿å­˜æ–‡æœ¬æŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("C++é¡¹ç›®ç¼–è¯‘ä¼˜åŒ–åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            # å†™å…¥æ‘˜è¦
            proj_info = summary['project_info']
            f.write("é¡¹ç›®æ‘˜è¦:\n")
            f.write(f"  é¡¹ç›®è·¯å¾„: {proj_info['path']}\n")
            f.write(f"  ç¼–è¯‘å™¨: {proj_info['compiler']}\n")
            f.write(f"  æ„å»ºç³»ç»Ÿ: {proj_info['build_system']}\n")
            f.write(f"  æ€»æ–‡ä»¶æ•°: {proj_info['total_files']}\n")
            f.write(f"  é¢„ä¼°ç¼–è¯‘æ—¶é—´: {proj_info['estimated_build_time']:.2f}s\n\n")
            
            # å†™å…¥é—®é¢˜
            if self.issues:
                f.write(f"æ£€æµ‹åˆ°çš„é—®é¢˜ ({len(self.issues)} ä¸ª):\n")
                for issue in self.issues:
                    f.write(f"  [{issue['severity'].name}] {issue['file']}\n")
                    f.write(f"     é—®é¢˜: {issue['message']}\n")
                    f.write(f"     å»ºè®®: {issue['suggestion']}\n\n")
            
            # å†™å…¥å»ºè®®
            if self.suggestions:
                f.write(f"ä¼˜åŒ–å»ºè®® ({len(self.suggestions)} ä¸ª):\n")
                for suggestion in self.suggestions:
                    f.write(f"  [{suggestion['priority'].name}] {suggestion['description']}\n")
                    f.write(f"     æ“ä½œ: {suggestion['action']}\n\n")
        
        print(f"ğŸ’¾ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
